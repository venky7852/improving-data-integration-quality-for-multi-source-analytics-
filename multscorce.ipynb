{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "pip install cloudant\n", "metadata": {"id": "829db23e-64ce-4d10-9d95-18141ec96e8e"}, "outputs": [{"name": "stdout", "text": "Collecting cloudant\n  Downloading cloudant-2.15.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: requests<3.0.0,>=2.7.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from cloudant) (2.32.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.7.0->cloudant) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.7.0->cloudant) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.7.0->cloudant) (1.26.19)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests<3.0.0,>=2.7.0->cloudant) (2025.1.31)\nDownloading cloudant-2.15.0-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: cloudant\nSuccessfully installed cloudant-2.15.0\nNote: you may need to restart the kernel to use updated packages.\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "code", "source": "from cloudant.client import Cloudant\n\nCLOUDANT_USERNAME = \"20a0796d-5c7b-4554-9977-7ee5a66ef7e6-bluemix\"\nCLOUDANT_API_KEY = \"pl03oZS2zrLUYF1afMovgghrPMyRmvcZ-gTyqjxB9IuA\"  # Use your API key here\nDATABASE_NAME = \"multi_source\"\n\n# Connect to Cloudant\nclient = Cloudant.iam(CLOUDANT_USERNAME, CLOUDANT_API_KEY, connect=True)\ndb = client[DATABASE_NAME]\n\nprint(\"Connected to Cloudant database:\", DATABASE_NAME)", "metadata": {"id": "87bbf379-d37b-4bbd-af74-9dc6e23369aa"}, "outputs": [{"name": "stdout", "text": "Connected to Cloudant database: multi_source\n", "output_type": "stream"}], "execution_count": 3}, {"cell_type": "code", "source": "CLOUDANT_USERNAME = \"20a0796d-5c7b-4554-9977-7ee5a66ef7e6-bluemix\"\nCLOUDANT_API_KEY = \"pl03oZS2zrLUYF1afMovgghrPMyRmvcZ-gTyqjxB9IuA\"  # Use your API key here\nDATABASE_NAME = \"multi_source\"\nCLOUDANT_SERVICE_URL=\"https://20a0796d-5c7b-4554-9977-7ee5a66ef7e6-bluemix.cloudantnosqldb.appdomain.cloud\"\n\n# Connect to Cloudant\nclient = Cloudant.iam(CLOUDANT_USERNAME, CLOUDANT_API_KEY, connect=True)\ndb = client[DATABASE_NAME]\n\n# Print all document IDs in the database\nprint([doc['_id'] for doc in db])", "metadata": {"id": "a242e10b-d314-4845-814c-98ae565e53e0"}, "outputs": [{"name": "stdout", "text": "['26d73a26485f3ba1a447d3e37b3f1c54', '2eec21445ba17562c4dd86c10cc4f4b0', '31967b3ef2d0ccc5e204b3a092efbd11', '4246bfcc8b2156b7f5837031050a559d', '55aa82ef1ead83925125b66820022fec', '55aa82ef1ead83925125b668200243c4', 'bb193a4a6e009e995e145223c62030c1', 'bb193a4a6e009e995e145223c620659a', 'cf827d11785b6e243ae09063a34998da', 'd8f7540ddd0809b36621925d00f303be']\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport dedupe\n\n# Sample multi-source datasets\ndata1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['John Doe', 'Jane Smith', 'Alice Brown'], 'Email': ['jdoe@email.com', 'jsmith@email.com', 'alice@email.com']})\ndata2 = pd.DataFrame({'ID': [101, 102, 103], 'Name': ['Jon Doe', 'J. Smith', 'Alicia Brown'], 'Email': ['johnd@email.com', 'janesmith@email.com', 'aliceb@email.com']})\n\n# Function to perform fuzzy matching\ndef fuzzy_match(name, choices, threshold=80):\n    match, score = process.extractOne(name, choices)\n    return match if score >= threshold else None\n\n# Apply fuzzy matching on names\ndata2['Matched Name'] = data2['Name'].apply(lambda x: fuzzy_match(x, data1['Name']))\n\n# Deduplication using Dedupe library\ndef deduplicate_data(data):\n    fields = [{'field': 'Name', 'type': 'String'}, {'field': 'Email', 'type': 'String'}]\n    deduper = dedupe.Dedupe(fields)\n    data_dict = data.to_dict(orient='records')\n    deduper.sample(data_dict, 50)\n    deduper.train()\n    clustered_dupes = deduper.partition(data_dict, threshold=0.5)\n    return clustered_dupes\n\n# Run deduplication\nclustered_data = deduplicate_data(pd.concat([data1, data2]))\n\n# Save cleaned dataset\ncleaned_df = pd.concat([data1, data2]).drop_duplicates()\ncleaned_df.to_csv(\"cleaned_dataset.csv\", index=False)\n\nprint(\"Data integration and cleaning complete. Cleaned dataset saved as 'cleaned_dataset.csv'.\")\n", "metadata": {"id": "22f7781d-93b4-4152-9834-252221de70af"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdedupe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Sample multi-source datasets\u001b[39;00m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'fuzzywuzzy'", "output_type": "error"}], "execution_count": 5}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport dedupe\n\n# Load dataset\nfile_path = \"/mnt/data/cleaned_multi_source_dataset (1).csv\"\ndata = pd.read_csv(file_path)\n\n# Function to perform fuzzy matching\ndef fuzzy_match(name, choices, threshold=80):\n    match, score = process.extractOne(name, choices)\n    return match if score >= threshold else None\n\n# Apply fuzzy matching to find duplicates\ndata['Matched Name'] = data['Name'].apply(lambda x: fuzzy_match(x, data['Name']))\n\n# Deduplication using Dedupe library\ndef deduplicate_data(data):\n    fields = [{'field': 'Name', 'type': 'String'}, {'field': 'Email', 'type': 'String'}]\n    deduper = dedupe.Dedupe(fields)\n    data_dict = data.to_dict(orient='records')\n    deduper.sample(data_dict, 50)\n    deduper.train()\n    clustered_dupes = deduper.partition(data_dict, threshold=0.5)\n    return clustered_dupes\n\n# Run deduplication\nclustered_data = deduplicate_data(data)\n\n# Save cleaned dataset\ncleaned_df = data.drop_duplicates()\ncleaned_df.to_csv(\"/mnt/data/cleaned_integrated_dataset.csv\", index=False)\n\nprint(\"AI-driven data integration complete. Cleaned dataset saved as 'cleaned_integrated_dataset.csv'.\")\n", "metadata": {"id": "4aa3cb65-1817-4e89-83ed-3585b8ef0b10"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdedupe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'fuzzywuzzy'", "output_type": "error"}], "execution_count": 6}, {"cell_type": "code", "source": "!pip install fuzzywuzzy python-Levenshtein\n", "metadata": {"id": "79631543-e8d4-4b05-a854-e59ff69f87e1"}, "outputs": [{"name": "stdout", "text": "Collecting fuzzywuzzy\n  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\nCollecting python-Levenshtein\n  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.26.1 (from python-Levenshtein)\n  Downloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n  Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\nDownloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.26.1 fuzzywuzzy-0.18.0 python-Levenshtein-0.26.1 rapidfuzz-3.12.1\n", "output_type": "stream"}], "execution_count": 7}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport dedupe\n\n# Load dataset\nfile_path = \"/mnt/data/cleaned_multi_source_dataset (1).csv\"\ndata = pd.read_csv(file_path)\n\n# Function to perform fuzzy matching\ndef fuzzy_match(name, choices, threshold=80):\n    match, score = process.extractOne(name, choices)\n    return match if score >= threshold else None\n\n# Apply fuzzy matching to find duplicates\ndata['Matched Name'] = data['Name'].apply(lambda x: fuzzy_match(x, data['Name']))\n\n# Deduplication using Dedupe library\ndef deduplicate_data(data):\n    fields = [{'field': 'Name', 'type': 'String'}, {'field': 'Email', 'type': 'String'}]\n    deduper = dedupe.Dedupe(fields)\n    data_dict = data.to_dict(orient='records')\n    deduper.sample(data_dict, 50)\n    deduper.train()\n    clustered_dupes = deduper.partition(data_dict, threshold=0.5)\n    return clustered_dupes\n\n# Run deduplication\nclustered_data = deduplicate_data(data)\n\n# Save cleaned dataset\ncleaned_df = data.drop_duplicates()\ncleaned_df.to_csv(\"/mnt/data/cleaned_integrated_dataset.csv\", index=False)\n\nprint(\"AI-driven data integration complete. Cleaned dataset saved as 'cleaned_integrated_dataset.csv'.\")\n", "metadata": {"id": "2ac89765-089f-47bb-8e22-58affb81881c"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdedupe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/cleaned_multi_source_dataset (1).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dedupe'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'dedupe'", "output_type": "error"}], "execution_count": 8}, {"cell_type": "code", "source": "!pip install dedupe\n", "metadata": {"id": "0af1bd1e-0d55-4a2e-9ce6-13dcc97edb72"}, "outputs": [{"name": "stdout", "text": "Collecting dedupe\n  Downloading dedupe-3.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from dedupe) (1.3.0)\nCollecting affinegap>=1.3 (from dedupe)\n  Downloading affinegap-1.12.tar.gz (33 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting categorical-distance>=1.9 (from dedupe)\n  Downloading categorical_distance-1.9-py3-none-any.whl.metadata (939 bytes)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from dedupe) (1.26.4)\nCollecting doublemetaphone (from dedupe)\n  Downloading DoubleMetaphone-1.1.tar.gz (34 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting highered>=0.2.0 (from dedupe)\n  Downloading highered-0.2.1-py2.py3-none-any.whl.metadata (977 bytes)\nCollecting simplecosine>=1.2 (from dedupe)\n  Downloading simplecosine-1.2-py2.py3-none-any.whl.metadata (912 bytes)\nCollecting haversine>=0.4.1 (from dedupe)\n  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\nCollecting BTrees>=4.1.4 (from dedupe)\n  Downloading BTrees-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting zope.index (from dedupe)\n  Downloading zope.index-7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting dedupe-Levenshtein-search (from dedupe)\n  Downloading dedupe_Levenshtein_search-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (504 bytes)\nCollecting persistent>=4.1.0 (from BTrees>=4.1.4->dedupe)\n  Downloading persistent-6.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nCollecting zope.interface>=5.0.0 (from BTrees>=4.1.4->dedupe)\n  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyhacrf-datamade>=0.2.0 (from highered>=0.2.0->dedupe)\n  Downloading pyhacrf_datamade-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: scipy>=1.5.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn->dedupe) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn->dedupe) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn->dedupe) (2.2.0)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from zope.index->dedupe) (72.1.0)\nCollecting zope.deferredimport (from persistent>=4.1.0->BTrees>=4.1.4->dedupe)\n  Downloading zope.deferredimport-5.0-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: cffi in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from persistent>=4.1.0->BTrees>=4.1.4->dedupe) (1.15.1)\nCollecting PyLBFGS>=0.1.3 (from pyhacrf-datamade>=0.2.0->highered>=0.2.0->dedupe)\n  Downloading PyLBFGS-0.2.0.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nRequirement already satisfied: pycparser in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from cffi->persistent>=4.1.0->BTrees>=4.1.4->dedupe) (2.21)\nCollecting zope.proxy (from zope.deferredimport->persistent>=4.1.0->BTrees>=4.1.4->dedupe)\n  Downloading zope.proxy-6.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading dedupe-3.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading BTrees-6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading categorical_distance-1.9-py3-none-any.whl (3.3 kB)\nDownloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\nDownloading highered-0.2.1-py2.py3-none-any.whl (3.3 kB)\nDownloading simplecosine-1.2-py2.py3-none-any.whl (3.2 kB)\nDownloading dedupe_Levenshtein_search-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading zope.index-7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading persistent-6.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m232.8/232.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyhacrf_datamade-0.2.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PyLBFGS-0.2.0.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m347.3/347.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading zope.deferredimport-5.0-py3-none-any.whl (10.0 kB)\nDownloading zope.proxy-6.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: affinegap, doublemetaphone\n  Building wheel for affinegap (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for affinegap: filename=affinegap-1.12-cp311-cp311-linux_x86_64.whl size=24972 sha256=e846368b62e2fc8766d37abda0cb2dec932483e7a0e449ffeb6c69fa8e667bcf\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/32/65/6e/bf13d087abec1151492a77d80aa9f797b7ebbd311b6d624bdc\n  Building wheel for doublemetaphone (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for doublemetaphone: filename=DoubleMetaphone-1.1-cp311-cp311-linux_x86_64.whl size=35920 sha256=58ce37a2981ad278c49390e9b941a7980fbd2dfa29a01df3a2a31500eaec4032\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/23/c1/3e/dcad9a2ac425c82d9ecb69bb5fe94784de3a134de417a804fc\nSuccessfully built affinegap doublemetaphone\nInstalling collected packages: doublemetaphone, dedupe-Levenshtein-search, affinegap, zope.interface, simplecosine, PyLBFGS, haversine, categorical-distance, zope.proxy, pyhacrf-datamade, zope.deferredimport, highered, persistent, BTrees, zope.index, dedupe\nSuccessfully installed BTrees-6.1 PyLBFGS-0.2.0.16 affinegap-1.12 categorical-distance-1.9 dedupe-3.0.3 dedupe-Levenshtein-search-1.4.5 doublemetaphone-1.1 haversine-2.9.0 highered-0.2.1 persistent-6.1 pyhacrf-datamade-0.2.8 simplecosine-1.2 zope.deferredimport-5.0 zope.index-7.0 zope.interface-7.2 zope.proxy-6.1\n", "output_type": "stream"}], "execution_count": 9}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport dedupe\n\n# Load dataset\nfile_path = \"/mnt/data/cleaned_multi_source_dataset (1).csv\"\ndata = pd.read_csv(file_path)\n\n# Function to perform fuzzy matching\ndef fuzzy_match(name, choices, threshold=80):\n    match, score = process.extractOne(name, choices)\n    return match if score >= threshold else None\n\n# Apply fuzzy matching to find duplicates\ndata['Matched Name'] = data['Name'].apply(lambda x: fuzzy_match(x, data['Name']))\n\n# Deduplication using Dedupe library\ndef deduplicate_data(data):\n    fields = [{'field': 'Name', 'type': 'String'}, {'field': 'Email', 'type': 'String'}]\n    deduper = dedupe.Dedupe(fields)\n    data_dict = data.to_dict(orient='records')\n    deduper.sample(data_dict, 50)\n    deduper.train()\n    clustered_dupes = deduper.partition(data_dict, threshold=0.5)\n    return clustered_dupes\n\n# Run deduplication\nclustered_data = deduplicate_data(data)\n\n# Save cleaned dataset\ncleaned_df = data.drop_duplicates()\ncleaned_df.to_csv(\"/mnt/data/cleaned_integrated_dataset.csv\", index=False)\n\nprint(\"AI-driven data integration complete. Cleaned dataset saved as 'cleaned_integrated_dataset.csv'.\")\n", "metadata": {"id": "45a4a6ef-7f51-41ab-8ffc-4e5e87ba70a9"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/cleaned_multi_source_dataset (1).csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Function to perform fuzzy matching\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuzzy_match\u001b[39m(name, choices, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m):\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/cleaned_multi_source_dataset (1).csv'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/cleaned_multi_source_dataset (1).csv'", "output_type": "error"}], "execution_count": 10}, {"cell_type": "code", "source": "import os\nprint(os.listdir(\"/mnt/data/\"))\n", "metadata": {"id": "b03afe76-be7a-4457-b6a9-79899a68276d"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/'", "output_type": "error"}], "execution_count": 11}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport dedupe\n\n# Load dataset\nfile_path = \"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\"\ndata = pd.read_csv(file_path)\n\n# Function to perform fuzzy matching\ndef fuzzy_match(name, choices, threshold=80):\n    match, score = process.extractOne(name, choices)\n    return match if score >= threshold else None\n\n# Apply fuzzy matching to find duplicates\ndata['Matched Name'] = data['Name'].apply(lambda x: fuzzy_match(x, data['Name']))\n\n# Deduplication using Dedupe library\ndef deduplicate_data(data):\n    fields = [{'field': 'Name', 'type': 'String'}, {'field': 'Email', 'type': 'String'}]\n    deduper = dedupe.Dedupe(fields)\n    data_dict = data.to_dict(orient='records')\n    deduper.sample(data_dict, 50)\n    deduper.train()\n    clustered_dupes = deduper.partition(data_dict, threshold=0.5)\n    return clustered_dupes\n\n# Run deduplication\nclustered_data = deduplicate_data(data)\n\n# Save cleaned dataset\ncleaned_df = data.drop_duplicates()\ncleaned_df.to_csv(\"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\", index=False)\n\nprint(\"AI-driven data integration complete. Cleaned dataset saved as 'cleaned_integrated_dataset.csv'.\")\n", "metadata": {"id": "b7ea9ab7-9f39-43c6-b882-cf43d62ad728", "msg_id": "e9ffc027-f011-43ea-b44c-d127b7f7ee73"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport dedupe\n\n# Load dataset\nfile_path = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\"\ndata = pd.read_csv(file_path)\n\n# Function to perform fuzzy matching\ndef fuzzy_match(name, choices, threshold=80):\n    match, score = process.extractOne(name, choices)\n    return match if score >= threshold else None\n\n# Apply fuzzy matching to find duplicates\ndata['Matched Name'] = data['Name'].apply(lambda x: fuzzy_match(x, data['Name']))\n\n# Deduplication using Dedupe library\ndef deduplicate_data(data):\n    fields = [{'field': 'Name', 'type': 'String'}, {'field': 'Email', 'type': 'String'}]\n    deduper = dedupe.Dedupe(fields)\n    data_dict = data.to_dict(orient='records')\n    deduper.sample(data_dict, 50)\n    deduper.train()\n    clustered_dupes = deduper.partition(data_dict, threshold=0.5)\n    return clustered_dupes\n\n# Run deduplication\nclustered_data = deduplicate_data(data)\n\n# Save cleaned dataset\ncleaned_df = data.drop_duplicates()\ncleaned_df.to_csv(r\"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\", index=False)\n\nprint(\"AI-driven data integration complete. Cleaned dataset saved as 'cleaned_integrated_dataset.csv'.\")\n", "metadata": {"id": "e8472c13-5574-4788-a99d-29311a5df693"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAdmin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmulti_source_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Function to perform fuzzy matching\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuzzy_match\u001b[39m(name, choices, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m):\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\multi_source_dataset.csv'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\multi_source_dataset.csv'", "output_type": "error"}], "execution_count": 13}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport recordlinkage\n\n# Load dataset\ndataset_path = r'C:\\Users\\Admin\\OneDrive\\Desktop\\cleaned_multi_source_dataset.csv'\ndata = pd.read_csv(dataset_path)\n\n# Assuming dataset has columns: 'id', 'name', 'email', 'source'\nsource_a = data[data['source'] == 'A'].reset_index(drop=True)\nsource_b = data[data['source'] == 'B'].reset_index(drop=True)\n\n# Step 1: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(name, name_list, threshold=80):\n    match, score = process.extractOne(name, name_list, scorer=fuzz.ratio)\n    return match if score >= threshold else None\n\nsource_b['matched_name'] = source_b['name'].apply(lambda x: fuzzy_match(x, source_a['name'].tolist()))\n\n# Step 2: Similarity Joins using Record Linkage\nindexer = recordlinkage.Index()\nindexer.full()\ncandidate_links = indexer.index(source_a, source_b)\n\ncompare = recordlinkage.Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.string('email', 'email', method='jarowinkler', threshold=0.85)\n\nmatches = compare.compute(candidate_links, source_a, source_b)\nmatched_pairs = matches[matches.sum(axis=1) > 0].reset_index()\n\n# Display results\nprint(\"Fuzzy Matched Names:\")\nprint(source_b[['name', 'matched_name']])\n\nprint(\"\\nEntity Resolution Matches:\")\nprint(matched_pairs)\n", "metadata": {"id": "4c901e31-690b-4aa1-bb93-9eed0bd52f0b"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrecordlinkage\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'fuzzywuzzy'", "output_type": "error"}], "execution_count": 2}, {"cell_type": "code", "source": "!pip install fuzzywuzzy python-Levenshtein\n", "metadata": {"id": "50ab7429-4451-4119-83fa-384437d85b4a"}, "outputs": [{"name": "stdout", "text": "Collecting fuzzywuzzy\n  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\nCollecting python-Levenshtein\n  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.26.1 (from python-Levenshtein)\n  Downloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n  Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\nDownloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.26.1 fuzzywuzzy-0.18.0 python-Levenshtein-0.26.1 rapidfuzz-3.12.1\n", "output_type": "stream"}], "execution_count": 3}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport recordlinkage\n\n# Load dataset\ndataset_path = r'C:\\Users\\Admin\\OneDrive\\Desktop\\cleaned_multi_source_dataset.csv'\ndata = pd.read_csv(dataset_path)\n\n# Assuming dataset has columns: 'id', 'name', 'email', 'source'\nsource_a = data[data['source'] == 'A'].reset_index(drop=True)\nsource_b = data[data['source'] == 'B'].reset_index(drop=True)\n\n# Step 1: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(name, name_list, threshold=80):\n    match, score = process.extractOne(name, name_list, scorer=fuzz.ratio)\n    return match if score >= threshold else None\n\nsource_b['matched_name'] = source_b['name'].apply(lambda x: fuzzy_match(x, source_a['name'].tolist()))\n\n# Step 2: Similarity Joins using Record Linkage\nindexer = recordlinkage.Index()\nindexer.full()\ncandidate_links = indexer.index(source_a, source_b)\n\ncompare = recordlinkage.Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.string('email', 'email', method='jarowinkler', threshold=0.85)\n\nmatches = compare.compute(candidate_links, source_a, source_b)\nmatched_pairs = matches[matches.sum(axis=1) > 0].reset_index()\n\n# Display results\nprint(\"Fuzzy Matched Names:\")\nprint(source_b[['name', 'matched_name']])\n\nprint(\"\\nEntity Resolution Matches:\")\nprint(matched_pairs)\n", "metadata": {"id": "f3cc108f-78d3-4665-a327-cfb75b1538d3"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrecordlinkage\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAdmin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcleaned_multi_source_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'recordlinkage'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'recordlinkage'", "output_type": "error"}], "execution_count": 4}, {"cell_type": "code", "source": "!pip install recordlinkage\n", "metadata": {"id": "015e673a-4103-4923-9344-42b5afbf8d69"}, "outputs": [{"name": "stdout", "text": "Collecting recordlinkage\n  Downloading recordlinkage-0.16-py3-none-any.whl.metadata (8.1 kB)\nCollecting jellyfish>=1 (from recordlinkage)\n  Downloading jellyfish-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\nRequirement already satisfied: numpy>=1.13 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.26.4)\nRequirement already satisfied: pandas<3,>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (2.1.4)\nRequirement already satisfied: scipy>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.11.4)\nRequirement already satisfied: scikit-learn>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.3.0)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.3.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2023.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn>=1->recordlinkage) (2.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\nDownloading recordlinkage-0.16-py3-none-any.whl (926 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m926.9/926.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jellyfish-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m347.2/347.2 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jellyfish, recordlinkage\nSuccessfully installed jellyfish-1.1.3 recordlinkage-0.16\n", "output_type": "stream"}], "execution_count": 5}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport recordlinkage\n\n# Load dataset\ndataset_path = '/mnt/data/cleaned_multi_source_dataset.csv'\n\ndata = pd.read_csv(dataset_path)\n\n# Assuming dataset has columns: 'id', 'name', 'email', 'source'\nsource_a = data[data['source'] == 'A'].reset_index(drop=True)\nsource_b = data[data['source'] == 'B'].reset_index(drop=True)\n\n# Step 1: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(name, name_list, threshold=80):\n    match, score = process.extractOne(name, name_list, scorer=fuzz.ratio)\n    return match if score >= threshold else None\n\nsource_b['matched_name'] = source_b['name'].apply(lambda x: fuzzy_match(x, source_a['name'].tolist()))\n\n# Step 2: Similarity Joins using Record Linkage\nindexer = recordlinkage.Index()\nindexer.full()\ncandidate_links = indexer.index(source_a, source_b)\n\ncompare = recordlinkage.Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.string('email', 'email', method='jarowinkler', threshold=0.85)\n\nmatches = compare.compute(candidate_links, source_a, source_b)\nmatched_pairs = matches[matches.sum(axis=1) > 0].reset_index()\n\n# Display results\nprint(\"Fuzzy Matched Names:\")\nprint(source_b[['name', 'matched_name']])\n\nprint(\"\\nEntity Resolution Matches:\")\nprint(matched_pairs)\n", "metadata": {"id": "56e0a084-c04b-4ef9-ac73-bfeb8aa07430"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/cleaned_multi_source_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming dataset has columns: 'id', 'name', 'email', 'source'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m source_a \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/cleaned_multi_source_dataset.csv'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/cleaned_multi_source_dataset.csv'", "output_type": "error"}], "execution_count": 7}, {"cell_type": "code", "source": "import os\n\nfile_path = \"/mnt/data/cleaned_multi_source_dataset.csv\"\nprint(\"File exists:\", os.path.exists(file_path))\n", "metadata": {"id": "0b1ff49c-f32f-4468-8d26-5f67b5eb1a17"}, "outputs": [{"name": "stdout", "text": "File exists: False\n", "output_type": "stream"}], "execution_count": 8}, {"cell_type": "code", "source": "import os\n\ndataset_path = \"/mnt/data/cleaned_multi_source_dataset.csv\"\n\nif os.path.exists(dataset_path):\n    print(\"File exists.\")\nelse:\n    print(\"File not found.\")\n", "metadata": {"id": "8134d191-3e3a-4414-b6bd-4fc596e31707"}, "outputs": [{"name": "stdout", "text": "File not found.\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "code", "source": "from google.colab import files\nuploaded = files.upload()\n", "metadata": {"id": "838dd606-1fcb-42b0-a0b0-152e607360c3"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      2\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'google.colab'", "output_type": "error"}], "execution_count": 2}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport recordlinkage\n\n# Load dataset\ndataset_path = '/mnt/data/cleaned_multi_source_dataset.csv'\n\ndata = pd.read_csv(dataset_path)\n\n# Assuming dataset has columns: 'id', 'name', 'email', 'source'\nsource_a = data[data['source'] == 'A'].reset_index(drop=True)\nsource_b = data[data['source'] == 'B'].reset_index(drop=True)\n\n# Step 1: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(name, name_list, threshold=80):\n    match, score = process.extractOne(name, name_list, scorer=fuzz.ratio)\n    return match if score >= threshold else None\n\nsource_b['matched_name'] = source_b['name'].apply(lambda x: fuzzy_match(x, source_a['name'].tolist()))\n\n# Step 2: Similarity Joins using Record Linkage\nindexer = recordlinkage.Index()\nindexer.full()\ncandidate_links = indexer.index(source_a, source_b)\n\ncompare = recordlinkage.Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.string('email', 'email', method='jarowinkler', threshold=0.85)\n\nmatches = compare.compute(candidate_links, source_a, source_b)\nmatched_pairs = matches[matches.sum(axis=1) > 0].reset_index()\n\n# Display results\nprint(\"Fuzzy Matched Names:\")\nprint(source_b[['name', 'matched_name']])\n\nprint(\"\\nEntity Resolution Matches:\")\nprint(matched_pairs)\n", "metadata": {"id": "2cba182c-4122-4c32-ac82-6ba3c1cd6119"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrecordlinkage\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'fuzzywuzzy'", "output_type": "error"}], "execution_count": 3}, {"cell_type": "code", "source": "!pip install fuzzywuzzy recordlinkage\n", "metadata": {"id": "24ff489a-11c4-43b5-a514-36ee8b003bbf"}, "outputs": [{"name": "stdout", "text": "Collecting fuzzywuzzy\n  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\nCollecting recordlinkage\n  Downloading recordlinkage-0.16-py3-none-any.whl.metadata (8.1 kB)\nCollecting jellyfish>=1 (from recordlinkage)\n  Downloading jellyfish-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\nRequirement already satisfied: numpy>=1.13 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.26.4)\nRequirement already satisfied: pandas<3,>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (2.1.4)\nRequirement already satisfied: scipy>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.11.4)\nRequirement already satisfied: scikit-learn>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.3.0)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.3.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas<3,>=1->recordlinkage) (2023.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn>=1->recordlinkage) (2.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1->recordlinkage) (1.16.0)\nDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\nDownloading recordlinkage-0.16-py3-none-any.whl (926 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m926.9/926.9 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jellyfish-1.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m347.2/347.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fuzzywuzzy, jellyfish, recordlinkage\nSuccessfully installed fuzzywuzzy-0.18.0 jellyfish-1.1.3 recordlinkage-0.16\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "code", "source": "!pip install python-levenshtein\n", "metadata": {"id": "de6ef4ba-b5fd-4205-b16d-93cb1cede2c3"}, "outputs": [{"name": "stdout", "text": "Collecting python-levenshtein\n  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.26.1 (from python-levenshtein)\n  Downloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-levenshtein)\n  Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-levenshtein\nSuccessfully installed Levenshtein-0.26.1 python-levenshtein-0.26.1 rapidfuzz-3.12.1\n", "output_type": "stream"}], "execution_count": 5}, {"cell_type": "code", "source": "from fuzzywuzzy import fuzz, process\n", "metadata": {"id": "e2ccf4a6-476f-4866-9f83-dd23540fceb3"}, "outputs": [], "execution_count": 1}, {"cell_type": "code", "source": "import pkg_resources\ninstalled_packages = [pkg.key for pkg in pkg_resources.working_set]\nprint(\"fuzzywuzzy\" in installed_packages)  # Should print True\n", "metadata": {"id": "1a984b1b-ddc0-4494-9dec-6395477a1ec0"}, "outputs": [{"name": "stdout", "text": "True\n", "output_type": "stream"}], "execution_count": 2}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz, process\nimport recordlinkage\n\n# Load dataset\ndataset_path = '/mnt/data/cleaned_multi_source_dataset.csv'\n\ndata = pd.read_csv(dataset_path)\n\n# Assuming dataset has columns: 'id', 'name', 'email', 'source'\nsource_a = data[data['source'] == 'A'].reset_index(drop=True)\nsource_b = data[data['source'] == 'B'].reset_index(drop=True)\n\n# Step 1: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(name, name_list, threshold=80):\n    match, score = process.extractOne(name, name_list, scorer=fuzz.ratio)\n    return match if score >= threshold else None\n\nsource_b['matched_name'] = source_b['name'].apply(lambda x: fuzzy_match(x, source_a['name'].tolist()))\n\n# Step 2: Similarity Joins using Record Linkage\nindexer = recordlinkage.Index()\nindexer.full()\ncandidate_links = indexer.index(source_a, source_b)\n\ncompare = recordlinkage.Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.string('email', 'email', method='jarowinkler', threshold=0.85)\n\nmatches = compare.compute(candidate_links, source_a, source_b)\nmatched_pairs = matches[matches.sum(axis=1) > 0].reset_index()\n\n# Display results\nprint(\"Fuzzy Matched Names:\")\nprint(source_b[['name', 'matched_name']])\n\nprint(\"\\nEntity Resolution Matches:\")\nprint(matched_pairs)\n", "metadata": {"id": "cc50e94e-3b31-4ca0-a8fb-af36a10de49d"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/cleaned_multi_source_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming dataset has columns: 'id', 'name', 'email', 'source'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m source_a \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/cleaned_multi_source_dataset.csv'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/cleaned_multi_source_dataset.csv'", "output_type": "error"}], "execution_count": 3}, {"cell_type": "code", "source": "import os\n\ndataset_path = \"/mnt/data/cleaned_multi_source_dataset.csv\"\n\nif os.path.exists(dataset_path):\n    print(\"File exists.\")\nelse:\n    print(\"File not found. Check the file path.\")\n", "metadata": {"id": "bbb074a2-7137-4707-a88e-c78043bf8ac3"}, "outputs": [{"name": "stdout", "text": "File not found. Check the file path.\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "code", "source": "print(os.listdir(\"/mnt/data/\"))\n", "metadata": {"id": "37644403-b87c-4e9a-b7cf-7ba19acd505d"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/'", "output_type": "error"}], "execution_count": 5}, {"cell_type": "code", "source": "import os\nprint(\"Current directory:\", os.getcwd())  # Print current working directory\nprint(\"Files and folders:\", os.listdir(\".\"))  # List files in the current directory\n", "metadata": {"id": "e4047768-884a-4611-87f5-5dc2be2fa6d3"}, "outputs": [{"name": "stdout", "text": "Current directory: /home/wsuser/work\nFiles and folders: []\n", "output_type": "stream"}], "execution_count": 6}, {"cell_type": "code", "source": "print(os.listdir(\"/\"))\n", "metadata": {"id": "382e1f1a-cf25-445a-8642-389c6a2d1635"}, "outputs": [{"name": "stdout", "text": "['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\n", "output_type": "stream"}], "execution_count": 7}, {"cell_type": "code", "source": "for root, dirs, files in os.walk(\"/\"):\n    if \"cleaned_multi_source_dataset.csv\" in files:\n        print(\"File found at:\", os.path.join(root, \"cleaned_multi_source_dataset.csv\"))\n", "metadata": {"id": "01ffca90-b0f6-45a7-a779-a86a1103ea09"}, "outputs": [], "execution_count": 9}, {"cell_type": "code", "source": "os.makedirs(\"/mnt/data/\", exist_ok=True)\n", "metadata": {"id": "d35b0ed0-195d-4a13-8eb6-7c2286f4894b"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)", "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n", "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n", "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/mnt/data/'"], "ename": "PermissionError", "evalue": "[Errno 13] Permission denied: '/mnt/data/'", "output_type": "error"}], "execution_count": 10}, {"cell_type": "code", "source": "import os\nprint(\"Current working directory:\", os.getcwd())  # Check where you are\nprint(\"Available directories:\", os.listdir(\"/\"))  # List top-level folders\n", "metadata": {"id": "9f882aaf-d285-4e9e-abc6-7ef594afb851"}, "outputs": [{"name": "stdout", "text": "Current working directory: /home/wsuser/work\nAvailable directories: ['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\n", "output_type": "stream"}], "execution_count": 11}, {"cell_type": "code", "source": "data_dir = os.path.expanduser(\"~/data\")  # Creates 'data' folder in home directory\nos.makedirs(data_dir, exist_ok=True)\nprint(\"Data directory:\", data_dir)\n", "metadata": {"id": "c3af1139-6431-434a-ae84-ea08602c2e3e"}, "outputs": [{"name": "stdout", "text": "Data directory: /home/wsuser/data\n", "output_type": "stream"}], "execution_count": 12}, {"cell_type": "code", "source": "dataset_path = os.path.join(data_dir, \"cleaned_multi_source_dataset.csv\")\n", "metadata": {"id": "0d0dd521-7486-4234-8353-cc210b5dd643"}, "outputs": [], "execution_count": 13}, {"cell_type": "code", "source": "print(os.listdir(\"/\"))\n", "metadata": {"id": "a1bf37bd-4f41-4782-bfe8-5f89c2e09a3c"}, "outputs": [{"name": "stdout", "text": "['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\n", "output_type": "stream"}], "execution_count": 14}, {"cell_type": "code", "source": "dataset_path = \"/project_data/cleaned_multi_source_dataset.csv\"\n", "metadata": {"id": "1b549eee-4e5d-42ef-84a6-a8210bf0bbe3"}, "outputs": [], "execution_count": 15}, {"cell_type": "code", "source": "import os\nprint(oct(os.stat(dataset_path).st_mode))  # Print file permissions\n", "metadata": {"id": "bc8290a2-181d-4e8a-adca-2822b0e28977"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28moct\u001b[39m(os\u001b[38;5;241m.\u001b[39mstat(dataset_path)\u001b[38;5;241m.\u001b[39mst_mode))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/project_data/cleaned_multi_source_dataset.csv'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/project_data/cleaned_multi_source_dataset.csv'", "output_type": "error"}], "execution_count": 16}, {"cell_type": "code", "source": "import os\n\ndataset_path = \"/project_data/cleaned_multi_source_dataset.csv\"\n\nif os.path.exists(dataset_path):\n    print(\"\u2705 File exists.\")\nelse:\n    print(\"\u274c File not found. Check the file path.\")\n", "metadata": {"id": "f98df0df-6ed3-4a09-ae67-bd378d2b6fcc"}, "outputs": [{"name": "stdout", "text": "\u274c File not found. Check the file path.\n", "output_type": "stream"}], "execution_count": 17}, {"cell_type": "code", "source": "print(os.listdir(\"/\"))\nif \"project_data\" in os.listdir(\"/\"):\n    print(os.listdir(\"/project_data/\"))\nelse:\n    print(\"\u274c '/project_data/' directory does not exist.\")\n", "metadata": {"id": "4e211721-bfb7-41b8-9f3d-65f317882745"}, "outputs": [{"name": "stdout", "text": "['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\n\u274c '/project_data/' directory does not exist.\n", "output_type": "stream"}], "execution_count": 18}, {"cell_type": "code", "source": "print(os.listdir(\"/home/\"))\nprint(os.listdir(\"/datasets/\"))\nprint(os.listdir(\"/workspace/\"))\n", "metadata": {"id": "17b41312-79f0-4bfa-ab1f-bbcd1f1274a9"}, "outputs": [{"name": "stdout", "text": "['wsuser', 'wsmonitor', 'wsbuild']\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/datasets/'", "output_type": "error"}], "execution_count": 19}, {"cell_type": "code", "source": "import os\n\nprint(\"Root directories:\", os.listdir(\"/\"))  # List top-level directories\n\n# Check user home directory\nprint(\"Home directories:\", os.listdir(\"/home/\"))\n", "metadata": {"id": "91bd4fe2-eae8-491a-afbf-22c75fd1f176"}, "outputs": [{"name": "stdout", "text": "Root directories: ['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\nHome directories: ['wsuser', 'wsmonitor', 'wsbuild']\n", "output_type": "stream"}], "execution_count": 20}, {"cell_type": "code", "source": "for folder in [\"/home/\", \"/mnt/\", \"/workspace/\", \"/project_data/\"]:\n    if os.path.exists(folder):\n        print(f\"Contents of {folder}:\", os.listdir(folder))\n", "metadata": {"id": "85005e22-b772-4b5e-a1bc-ce5292a1be32"}, "outputs": [{"name": "stdout", "text": "Contents of /home/: ['wsuser', 'wsmonitor', 'wsbuild']\nContents of /mnt/: ['asset_file_api']\n", "output_type": "stream"}], "execution_count": 21}, {"cell_type": "code", "source": "!pip install pandas thefuzz recordlinkage\n", "metadata": {"id": "b48f9ea2-d449-4a0c-9981-a5d88a36f151"}, "outputs": [{"name": "stdout", "text": "Requirement already satisfied: pandas in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (2.1.4)\nCollecting thefuzz\n  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: recordlinkage in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (0.16)\nRequirement already satisfied: numpy<2,>=1.23.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pandas) (2023.3)\nRequirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from thefuzz) (3.12.1)\nRequirement already satisfied: jellyfish>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.1.3)\nRequirement already satisfied: scipy>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.11.4)\nRequirement already satisfied: scikit-learn>=1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.3.0)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from recordlinkage) (1.3.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn>=1->recordlinkage) (2.2.0)\nDownloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\nInstalling collected packages: thefuzz\nSuccessfully installed thefuzz-0.22.1\n", "output_type": "stream"}], "execution_count": 22}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import fuzz, process\nfrom recordlinkage import Compare, Index\n\n# Load dataset from IBM Cloud Jupyter Notebook\ndataset_path = '/mnt/data/cleaned_multi_source_dataset.csv'\n\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please check the file path.\")\n\n# Read dataset\ndata = pd.read_csv(dataset_path)\nprint(\"Dataset Loaded Successfully!\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for i, val in enumerate(df[col1]):\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\nfinal_data.to_csv('/mnt/data/integrated_dataset.csv', index=False)\nprint(\"\u2705 Data Integration Completed! Saved as 'integrated_dataset.csv'\")\n", "metadata": {"id": "4df4d398-9b0a-4368-be12-a105540353ae"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/cleaned_multi_source_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please check the file path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Read dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at /mnt/data/cleaned_multi_source_dataset.csv. Please check the file path."], "ename": "FileNotFoundError", "evalue": "Dataset not found at /mnt/data/cleaned_multi_source_dataset.csv. Please check the file path.", "output_type": "error"}], "execution_count": 23}, {"cell_type": "code", "source": "import os\nprint(os.listdir(\"/mnt/data/\"))\n", "metadata": {"id": "410750aa-e96f-4177-8ed1-2c90dba46b7a"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/'", "output_type": "error"}], "execution_count": 25}, {"cell_type": "code", "source": "import os\nprint(os.listdir(\"/\"))  # List root directories\nprint(os.listdir(\"/home/\"))  # Try home directory\n", "metadata": {"id": "48d30782-3b5b-4fe4-83db-d2ebc5d54fbf"}, "outputs": [{"name": "stdout", "text": "['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\n['wsuser', 'wsmonitor', 'wsbuild']\n", "output_type": "stream"}], "execution_count": 27}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path\ndataset_path = \"/mnt/data/multi_source_dataset.csv\"\n\n# Check if file exists\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please upload the correct file.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"\u2705 Dataset Loaded Successfully!\\n\", data.head())\n\n# \ud83d\udd39 Step 1: Data Cleaning & Normalization\ndef clean_text(text):\n    \"\"\"Convert text to lowercase and remove leading/trailing spaces.\"\"\"\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\n# Normalize columns\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\n# Remove duplicates\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# \ud83d\udd39 Step 2: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col, threshold=85):\n    \"\"\"Perform fuzzy matching on a column and return similar entries.\"\"\"\n    matches = []\n    for val in df[col]:\n        best_match, score = process.extractOne(val, df[col])\n        if score >= threshold and val != best_match:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col, 'matched_name', 'similarity_score'])\n\n# Apply fuzzy matching\nfuzzy_results = fuzzy_match(data, 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", fuzzy_results.head())\n\n# \ud83d\udd39 Step 3: Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\n# Extract duplicate records\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# \ud83d\udd39 Step 4: Merge Similar Records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = \"/mnt/data/integrated_dataset.csv\"\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "74f34a91-5883-4500-aa22-bc3b15796abd"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if file exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please upload the correct file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at /mnt/data/multi_source_dataset.csv. Please upload the correct file."], "ename": "FileNotFoundError", "evalue": "Dataset not found at /mnt/data/multi_source_dataset.csv. Please upload the correct file.", "output_type": "error"}], "execution_count": 28}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path\ndataset_path = \"/mnt/data/multi_source_dataset.csv\"\n\n# Check if file exists\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please upload the correct file.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"\u2705 Dataset Loaded Successfully!\\n\", data.head())\n\n# \ud83d\udd39 Step 1: Data Cleaning & Normalization\ndef clean_text(text):\n    \"\"\"Convert text to lowercase and remove leading/trailing spaces.\"\"\"\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\n# Normalize columns\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\n# Remove duplicates\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# \ud83d\udd39 Step 2: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col, threshold=85):\n    \"\"\"Perform fuzzy matching on a column and return similar entries.\"\"\"\n    matches = []\n    for val in df[col]:\n        best_match, score = process.extractOne(val, df[col])\n        if score >= threshold and val != best_match:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col, 'matched_name', 'similarity_score'])\n\n# Apply fuzzy matching\nfuzzy_results = fuzzy_match(data, 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", fuzzy_results.head())\n\n# \ud83d\udd39 Step 3: Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\n# Extract duplicate records\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# \ud83d\udd39 Step 4: Merge Similar Records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = \"/mnt/data/integrated_dataset.csv\"\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "d69492bd-7d87-4d90-a2fe-58b164de8835"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if file exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please upload the correct file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at /mnt/data/multi_source_dataset.csv. Please upload the correct file."], "ename": "FileNotFoundError", "evalue": "Dataset not found at /mnt/data/multi_source_dataset.csv. Please upload the correct file.", "output_type": "error"}], "execution_count": 29}, {"cell_type": "code", "source": "import os\nprint(os.listdir(\"/mnt/data/\"))  # List files in the directory\n", "metadata": {"id": "958dedff-a246-468a-9789-01bfcc68fa21"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/'", "output_type": "error"}], "execution_count": 30}, {"cell_type": "code", "source": "import os\nprint(os.listdir(\"/\"))  # List root-level directories\n", "metadata": {"id": "99bd176d-4425-4d28-8223-3bb52d35008e"}, "outputs": [{"name": "stdout", "text": "['dev', 'srv', 'afs', 'mnt', 'proc', 'root', 'media', 'tmp', 'sys', 'home', 'opt', 'boot', 'cachi2', 'var', 'run', 'lib64', 'bin', 'etc', 'usr', 'sbin', 'lib', 'licenses']\n", "output_type": "stream"}], "execution_count": 31}, {"cell_type": "code", "source": "print(os.listdir(\"/home/\"))\nprint(os.listdir(\"/workspace/\"))\nprint(os.listdir(\"/project_data/\"))\n", "metadata": {"id": "7cc4dcb3-8393-4e56-a503-d7070b749910"}, "outputs": [{"name": "stdout", "text": "['wsuser', 'wsmonitor', 'wsbuild']\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/project_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/workspace/'", "output_type": "error"}], "execution_count": 32}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path\ndataset_path = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\"\n# Check if file exists\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please upload the correct file.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"\u2705 Dataset Loaded Successfully!\\n\", data.head())\n\n# \ud83d\udd39 Step 1: Data Cleaning & Normalization\ndef clean_text(text):\n    \"\"\"Convert text to lowercase and remove leading/trailing spaces.\"\"\"\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\n# Normalize columns\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\n# Remove duplicates\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# \ud83d\udd39 Step 2: Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col, threshold=85):\n    \"\"\"Perform fuzzy matching on a column and return similar entries.\"\"\"\n    matches = []\n    for val in df[col]:\n        best_match, score = process.extractOne(val, df[col])\n        if score >= threshold and val != best_match:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col, 'matched_name', 'similarity_score'])\n\n# Apply fuzzy matching\nfuzzy_results = fuzzy_match(data, 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", fuzzy_results.head())\n\n# \ud83d\udd39 Step 3: Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\n# Extract duplicate records\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# \ud83d\udd39 Step 4: Merge Similar Records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = \"/mnt/data/integrated_dataset.csv\"\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "9c07083e-ecb8-4584-b666-109d0b8b2778"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Check if file exists\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please upload the correct file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv. Please upload the correct file."], "ename": "FileNotFoundError", "evalue": "Dataset not found at C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv. Please upload the correct file.", "output_type": "error"}], "execution_count": 36}, {"cell_type": "code", "source": "C:\\Users\\Admin\\OneDrive\\Desktop\\\n", "metadata": {"id": "ded2c44f-bd78-482e-9c95-57b872692b15"}, "outputs": [{"traceback": ["\u001b[0;36m  Cell \u001b[0;32mIn[35], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    C:\\Users\\Admin\\OneDrive\\Desktop\\\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"], "ename": "SyntaxError", "evalue": "unexpected character after line continuation character (1803474656.py, line 1)", "output_type": "error"}], "execution_count": 35}, {"cell_type": "code", "source": "import pandas as pd\nimport os\n\n# Corrected file path (Choose ONE of these options)\ndataset_path = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\"  # Option 1: Raw String\n# dataset_path = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\multi_source_dataset.csv\"  # Option 2: Double Backslashes\n# dataset_path = \"C:/Users/Admin/OneDrive/Desktop/multi_source_dataset.csv\"  # Option 3: Forward Slashes\n\n# Check if the file exists\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please upload the correct file.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"Dataset Loaded Successfully!\\n\", data.head())\n", "metadata": {"id": "33938cd2-6b5a-4b9d-badd-c686a6dfe59b", "scrolled": true}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# dataset_path = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\multi_source_dataset.csv\"  # Option 2: Double Backslashes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# dataset_path = \"C:/Users/Admin/OneDrive/Desktop/multi_source_dataset.csv\"  # Option 3: Forward Slashes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if the file exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please upload the correct file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv. Please upload the correct file."], "ename": "FileNotFoundError", "evalue": "Dataset not found at C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv. Please upload the correct file.", "output_type": "error"}], "execution_count": 37}, {"cell_type": "code", "source": "import pandas as pd\nimport os\n\n# Corrected dataset path (Choose the correct one)\ndataset_path = r\"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\"  # \u2705 Option 1: Raw String\n# dataset_path = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\multi_source_dataset.csv\"  # \u2705 Option 2: Double Backslashes\n# dataset_path = \"C:/Users/Admin/OneDrive/Desktop/multi_source_dataset.csv\"  # \u2705 Option 3: Forward Slashes\n\n# Check if the file exists\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"\u274c Dataset not found at {dataset_path}. Please check the file path and try again.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"\u2705 Dataset Loaded Successfully!\\n\", data.head())\n", "metadata": {"id": "5c0f45f7-a01a-40fb-8fe4-a92fa9c797ea"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[38], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# dataset_path = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\multi_source_dataset.csv\"  # \u2705 Option 2: Double Backslashes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# dataset_path = \"C:/Users/Admin/OneDrive/Desktop/multi_source_dataset.csv\"  # \u2705 Option 3: Forward Slashes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if the file exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u274c Dataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please check the file path and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: \u274c Dataset not found at C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv. Please check the file path and try again."], "ename": "FileNotFoundError", "evalue": "\u274c Dataset not found at C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv. Please check the file path and try again.", "output_type": "error"}], "execution_count": 38}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path for IBM Cloud Jupyter Notebook\ndataset_path = '/mnt/data/multi_source_dataset.csv'\n\n# Check if dataset exists\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"Dataset not found at {dataset_path}. Please check the file path.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"\u2705 Dataset Loaded Successfully!\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/mnt/data/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "05fbb027-00ac-469e-a6d3-454a3b5b8ce3"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if dataset exists\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please check the file path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found at /mnt/data/multi_source_dataset.csv. Please check the file path."], "ename": "FileNotFoundError", "evalue": "Dataset not found at /mnt/data/multi_source_dataset.csv. Please check the file path.", "output_type": "error"}], "execution_count": 1}, {"cell_type": "code", "source": "import os\nprint(os.listdir(\"/mnt/data/\"))\n", "metadata": {"id": "3b39daa9-1a83-4c0b-b722-15d0d1468039"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/'"], "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: '/mnt/data/'", "output_type": "error"}], "execution_count": 2}, {"cell_type": "code", "source": "from google.colab import files\nuploaded = files.upload()\n", "metadata": {"id": "4dc168d6-22d7-44c4-a0df-2b61a00cdd55"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      2\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'google.colab'", "output_type": "error"}], "execution_count": 3}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path for IBM Cloud Jupyter Notebook\ndataset_path = '/home/wsuser/work/multi_source_dataset.csv'  # Update path if needed\n\n# Check if dataset exists; prompt manual upload if missing\nif not os.path.exists(dataset_path):\n    raise FileNotFoundError(f\"\u26a0\ufe0f Dataset not found at {dataset_path}. Please upload the file manually.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(\"\u2705 Dataset Loaded Successfully!\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/home/wsuser/work/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "392143d9-7a77-4fe3-8cb9-7223a30b84fb"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check if dataset exists; prompt manual upload if missing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u26a0\ufe0f Dataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please upload the file manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: \u26a0\ufe0f Dataset not found at /home/wsuser/work/multi_source_dataset.csv. Please upload the file manually."], "ename": "FileNotFoundError", "evalue": "\u26a0\ufe0f Dataset not found at /home/wsuser/work/multi_source_dataset.csv. Please upload the file manually.", "output_type": "error"}], "execution_count": 4}, {"cell_type": "code", "source": "import os\nprint(\"Home Directory:\", os.listdir(\"/home/wsuser/work/\"))\n", "metadata": {"id": "e7346c7f-c653-47d8-a56d-dbd868ccb0b1"}, "outputs": [{"name": "stdout", "text": "Home Directory: []\n", "output_type": "stream"}], "execution_count": 5}, {"cell_type": "code", "source": "dataset_path = '/home/wsuser/work/multi_source_dataset.csv'\n", "metadata": {"id": "75e4a37d-bc1c-43ce-adee-080c28bef082"}, "outputs": [], "execution_count": 6}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define possible dataset paths\ndataset_paths = ['/mnt/data/multi_source_dataset.csv', '/home/wsuser/work/multi_source_dataset.csv']\n\n# Check which path exists\ndataset_path = None\nfor path in dataset_paths:\n    if os.path.exists(path):\n        dataset_path = path\n        break\n\nif not dataset_path:\n    raise FileNotFoundError(\"Dataset not found in expected locations. Please check the file path or upload the dataset.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/mnt/data/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "4ec6270b-245e-4f26-a023-4ff98eb63099"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_path:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found in expected locations. Please check the file path or upload the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found in expected locations. Please check the file path or upload the dataset."], "ename": "FileNotFoundError", "evalue": "Dataset not found in expected locations. Please check the file path or upload the dataset.", "output_type": "error"}], "execution_count": 7}, {"cell_type": "code", "source": "!ls -lh /mnt/data/ /home/wsuser/work/", "metadata": {"id": "deaba546-d36c-4a2b-bb2b-9b49b00def01"}, "outputs": [{"name": "stdout", "text": "ls: cannot access '/mnt/data/': No such file or directory\n/home/wsuser/work/:\ntotal 0\n", "output_type": "stream"}], "execution_count": 8}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define possible dataset paths\ndataset_paths = ['/mnt/data/multi_source_dataset.csv', '/home/wsuser/work/multi_source_dataset.csv']\n\n# Check which path exists\ndataset_path = next((path for path in dataset_paths if os.path.exists(path)), None)\n\n# If not found, prompt for manual upload\nif not dataset_path:\n    from ipywidgets import FileUpload\n    from io import BytesIO\n    \n    print(\"\u26a0\ufe0f Dataset not found! Please upload the dataset manually.\")\n    uploader = FileUpload()\n    display(uploader)\n\n    def save_uploaded_file(change):\n        global dataset_path\n        if uploader.value:\n            filename = list(uploader.value.keys())[0]\n            dataset_path = f\"/home/wsuser/work/{filename}\"\n            with open(dataset_path, \"wb\") as f:\n                f.write(uploader.value[filename][\"content\"])\n            print(f\"\u2705 Dataset uploaded and saved as: {dataset_path}\")\n\n    uploader.observe(save_uploaded_file, names='value')\n\n# Load dataset once uploaded\nif dataset_path:\n    data = pd.read_csv(dataset_path)\n    print(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Proceed with data integration steps (unchanged)\n", "metadata": {"id": "362f3422-5674-4487-9c5a-11887c89435d"}, "outputs": [{"name": "stdout", "text": "\u26a0\ufe0f Dataset not found! Please upload the dataset manually.\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "FileUpload(value=(), description='Upload')", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5237a109307445ea9334bd7c7574fbf8"}}, "metadata": {}}], "execution_count": 10}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path\ndataset_path = '/home/wsuser/work/multi_source_dataset.csv'\n\n# Check available files in directories\nprint(\"Checking available files in /home/wsuser/work/:\")\nprint(os.listdir(\"/home/wsuser/work/\"))\n\n# Prompt for file upload if dataset is missing\nif not os.path.exists(dataset_path):\n    print(\"\u26a0\ufe0f Dataset not found! Please manually upload 'multi_source_dataset.csv' using the Jupyter Notebook interface.\")\n\n# Wait for manual upload, then recheck\nwhile not os.path.exists(dataset_path):\n    input(\"\ud83d\udcc2 Please upload the dataset and press Enter to continue...\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/home/wsuser/work/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "fc2b40e7-df58-4b10-8811-9788296b79f5"}, "outputs": [{"name": "stdout", "text": "Checking available files in /home/wsuser/work/:\n[]\n\u26a0\ufe0f Dataset not found! Please manually upload 'multi_source_dataset.csv' using the Jupyter Notebook interface.\n", "output_type": "stream"}, {"output_type": "stream", "name": "stdin", "text": "\ud83d\udcc2 Please upload the dataset and press Enter to continue... \n\ud83d\udcc2 Please upload the dataset and press Enter to continue... \"C:\\Users\\Admin\\OneDrive\\Desktop\\multi_source_dataset.csv\"\n"}], "execution_count": 11}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path\ndataset_path = '/home/wsuser/work/multi_source_dataset.csv'\n\n# Check available files in directories\nprint(\"Checking available files in /home/wsuser/work/:\")\nprint(os.listdir(\"/home/wsuser/work/\"))\n\n# Prompt for file upload if dataset is missing\nif not os.path.exists(dataset_path):\n    print(\"\u26a0\ufe0f Dataset not found! Please manually upload 'multi_source_dataset.csv' using the Jupyter Notebook interface.\")\n\n# Wait for manual upload, then recheck\nwhile not os.path.exists(dataset_path):\n    input(\"\ud83d\udcc2 Please upload the dataset and press Enter to continue...\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/home/wsuser/work/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n\n", "metadata": {"id": "27825fc5-bfea-4f07-8ae3-ec3431b0cb40"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define possible dataset paths\ndataset_paths = ['/mnt/data/multi_source_dataset.csv', '/home/wsuser/work/multi_source_dataset.csv']\nfound_files = []\n\n# Check available files in the working directory\nworkspace_dir = \"/home/wsuser/work/\"\nif os.path.exists(workspace_dir):\n    found_files = os.listdir(workspace_dir)\n\n# If dataset not found, prompt for file upload\nif not any(os.path.exists(path) for path in dataset_paths):\n    print(\"\u26a0\ufe0f Dataset not found in expected locations.\")\n\n    # File upload widget\n    upload_widget = widgets.FileUpload(accept='.csv', multiple=False)\n    display(upload_widget)\n\n    # Wait for file upload\n    def on_upload(change):\n        uploaded_file = list(upload_widget.value.values())[0]\n        dataset_path = os.path.join(workspace_dir, uploaded_file['metadata']['name'])\n\n        # Save uploaded file\n        with open(dataset_path, 'wb') as f:\n            f.write(uploaded_file['content'])\n        \n        print(f\"\u2705 File uploaded successfully: {dataset_path}\")\n    \n    upload_widget.observe(on_upload, names='value')\n    raise FileNotFoundError(\"Please upload the dataset using the provided widget.\")\n\n# Select the correct dataset path\ndataset_path = next(path for path in dataset_paths if os.path.exists(path))\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/mnt/data/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "e4a3d122-50b5-407a-aaf2-0e1504c1ec71"}, "outputs": [{"name": "stdout", "text": "\u26a0\ufe0f Dataset not found in expected locations.\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "FileUpload(value=(), accept='.csv', description='Upload')", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b2b9d71afcc044dc90c552d5094cb1e4"}}, "metadata": {}}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[1], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u2705 File uploaded successfully: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     upload_widget\u001b[38;5;241m.\u001b[39mobserve(on_upload, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upload the dataset using the provided widget.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Select the correct dataset path\u001b[39;00m\n\u001b[1;32m     40\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m dataset_paths \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path))\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Please upload the dataset using the provided widget."], "ename": "FileNotFoundError", "evalue": "Please upload the dataset using the provided widget.", "output_type": "error"}], "execution_count": 1}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define possible dataset paths\ndataset_paths = ['/mnt/data/multi_source_dataset.csv', '/home/wsuser/work/multi_source_dataset.csv']\n\n# Check which path exists\ndataset_path = None\nfor path in dataset_paths:\n    if os.path.exists(path):\n        dataset_path = path\n        break\n\nif not dataset_path:\n    raise FileNotFoundError(\"Dataset not found in expected locations. Please check the file path or upload the dataset.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/mnt/data/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "1a4d9a5a-9e47-405d-83cc-8004695c4071"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_path:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found in expected locations. Please check the file path or upload the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found in expected locations. Please check the file path or upload the dataset."], "ename": "FileNotFoundError", "evalue": "Dataset not found in expected locations. Please check the file path or upload the dataset.", "output_type": "error"}], "execution_count": 2}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nimport ipywidgets as widgets\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\nfrom IPython.display import display\n\n# Define possible dataset paths\ndataset_paths = [\n    '/mnt/data/multi_source_dataset.csv',\n    '/home/wsuser/work/multi_source_dataset.csv'\n]\n\n# Function to list available files\ndef list_files(directory):\n    try:\n        return os.listdir(directory)\n    except FileNotFoundError:\n        return []\n\n# Debugging: Print available files in expected directories\nprint(f\"\ud83d\udcc2 Files in /mnt/data/: {list_files('/mnt/data/')}\")\nprint(f\"\ud83d\udcc2 Files in /home/wsuser/work/: {list_files('/home/wsuser/work/')}\")\n\n# Check which dataset path exists\ndataset_path = next((path for path in dataset_paths if os.path.exists(path)), None)\n\nif not dataset_path:\n    print(\"\u26a0\ufe0f Dataset not found! Please manually upload 'multi_source_dataset.csv' below.\")\n\n    # Display upload widget\n    upload_widget = widgets.FileUpload(accept='.csv', multiple=False)\n    display(upload_widget)\n\n    def on_upload(change):\n        uploaded_file = list(upload_widget.value.values())[0]\n        save_path = \"/home/wsuser/work/multi_source_dataset.csv\"\n        with open(save_path, \"wb\") as f:\n            f.write(uploaded_file['content'])\n        print(f\"\u2705 File uploaded successfully: {save_path}. Please re-run the notebook.\")\n    \n    upload_widget.observe(on_upload, names='value')\n    raise FileNotFoundError(\"Dataset not found. Please upload the file manually.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    if pd.isnull(text):\n        return \"\"\n    return str(text).strip().lower()\n\ndata['name'] = data['name'].apply(clean_text)\ndata['email'] = data['email'].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col1, col2, threshold=85):\n    \"\"\"Perform fuzzy matching on two columns and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col1]:\n        best_match, score = process.extractOne(val, df[col2])\n        if score >= threshold:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=[col1, col2, 'similarity_score'])\n\nentity_matches = fuzzy_match(data, 'name', 'name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('name', 'name', method='jarowinkler', threshold=0.85)\ncompare.exact('email', 'email')\ncomparison_result = compare.compute(pairs, data)\n\nduplicate_pairs = comparison_result[comparison_result.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records\nfinal_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", final_data.head())\n\n# Save the cleaned and integrated dataset\noutput_path = '/mnt/data/integrated_dataset.csv'\nfinal_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "d266a206-36ce-4bc1-90c8-afccfd4d5389"}, "outputs": [{"name": "stdout", "text": "\ud83d\udcc2 Files in /mnt/data/: []\n\ud83d\udcc2 Files in /home/wsuser/work/: []\n\u26a0\ufe0f Dataset not found! Please manually upload 'multi_source_dataset.csv' below.\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "FileUpload(value=(), accept='.csv', description='Upload')", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a0aafaaa68db4c11b36d17e3fc8fc2cf"}}, "metadata": {}}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u2705 File uploaded successfully: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please re-run the notebook.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     upload_widget\u001b[38;5;241m.\u001b[39mobserve(on_upload, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. Please upload the file manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     46\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found. Please upload the file manually."], "ename": "FileNotFoundError", "evalue": "Dataset not found. Please upload the file manually.", "output_type": "error"}], "execution_count": 4}, {"cell_type": "code", "source": "import pandas as pd\nimport ibm_boto3\nfrom ibm_botocore.client import Config\n\n# IBM Cloud Object Storage credentials (Replace with yours)\nCOS_ENDPOINT = \"https://s3.us.cloud-object-storage.appdomain.cloud\"  # Example endpoint\nCOS_API_KEY_ID = \"your-api-key\"  # Replace with your API Key\nCOS_INSTANCE_CRN = \"your-instance-crn\"  # Replace with your Instance CRN\nBUCKET_NAME = \"your-bucket-name\"  # Replace with your bucket name\nFILE_NAME = \"multi_source_dataset.csv\"  # Name of your file in the bucket\n\n# Connect to IBM Cloud Object Storage\ncos_client = ibm_boto3.client(\"s3\",\n    ibm_api_key_id=COS_API_KEY_ID,\n    ibm_service_instance_id=COS_INSTANCE_CRN,\n    config=Config(signature_version=\"oauth\"),\n    endpoint_url=COS_ENDPOINT\n)\n\n# Download file from IBM Cloud Object Storage\ncos_client.download_file(BUCKET_NAME, FILE_NAME, FILE_NAME)\n\n# Load dataset into Pandas DataFrame\ndf = pd.read_csv(FILE_NAME)\n\n# Process dataset (Example: Display first few rows)\nprint(df.head())\n\n# Example: Data Cleaning (Remove missing values)\ndf_cleaned = df.dropna()\n\n# Example: Save the processed data back\nprocessed_file_name = \"processed_dataset.csv\"\ndf_cleaned.to_csv(processed_file_name, index=False)\n\n# Upload processed file back to IBM Cloud\ncos_client.upload_file(processed_file_name, BUCKET_NAME, processed_file_name)\n\nprint(f\"Processed file '{processed_file_name}' uploaded\u00a0successfully!\")\n", "metadata": {"id": "e2fff8f0-c187-48c8-882c-43340992881d"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mCredentialRetrievalError\u001b[0m                  Traceback (most recent call last)", "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     13\u001b[0m cos_client \u001b[38;5;241m=\u001b[39m ibm_boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     ibm_api_key_id\u001b[38;5;241m=\u001b[39mCOS_API_KEY_ID,\n\u001b[1;32m     15\u001b[0m     ibm_service_instance_id\u001b[38;5;241m=\u001b[39mCOS_INSTANCE_CRN,\n\u001b[1;32m     16\u001b[0m     config\u001b[38;5;241m=\u001b[39mConfig(signature_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moauth\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m     endpoint_url\u001b[38;5;241m=\u001b[39mCOS_ENDPOINT\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Download file from IBM Cloud Object Storage\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m cos_client\u001b[38;5;241m.\u001b[39mdownload_file(BUCKET_NAME, FILE_NAME, FILE_NAME)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load dataset into Pandas DataFrame\u001b[39;00m\n\u001b[1;32m     24\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FILE_NAME)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_boto3/s3/inject.py:190\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download an S3 object to a file.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transfer\u001b[38;5;241m.\u001b[39mdownload_file(\n\u001b[1;32m    191\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mBucket,\n\u001b[1;32m    192\u001b[0m         key\u001b[38;5;241m=\u001b[39mKey,\n\u001b[1;32m    193\u001b[0m         filename\u001b[38;5;241m=\u001b[39mFilename,\n\u001b[1;32m    194\u001b[0m         extra_args\u001b[38;5;241m=\u001b[39mExtraArgs,\n\u001b[1;32m    195\u001b[0m         callback\u001b[38;5;241m=\u001b[39mCallback,\n\u001b[1;32m    196\u001b[0m     )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_boto3/s3/transfer.py:326\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    322\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m    323\u001b[0m     bucket, key, filename, extra_args, subscribers\n\u001b[1;32m    324\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from ibm_boto3 instead of\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# ibm_s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# catching the ibm_boto3 one instead of the ibm_s3transfer exception to do\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3TransferRetriesExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_s3transfer/futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_s3transfer/tasks.py:269\u001b[0m, in \u001b[0;36mSubmissionTask._main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mset_status_to_running()\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Call the submit method to start submitting tasks to execute the\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# transfer.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit(transfer_future\u001b[38;5;241m=\u001b[39mtransfer_future, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# If there was an exception raised during the submission of task\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# there is a chance that the final task that signals if a transfer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# Set the exception, that caused the process to fail.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_s3transfer/download.py:354\u001b[0m, in \u001b[0;36mDownloadSubmissionTask._submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m:param client: The client associated with the transfer manager\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    downloading streams\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;66;03m# If a size was not provided figure out the size for the\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;66;03m# user.\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mhead_object(\n\u001b[1;32m    355\u001b[0m         Bucket\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mbucket,\n\u001b[1;32m    356\u001b[0m         Key\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mkey,\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mextra_args,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m     transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mprovide_transfer_size(\n\u001b[1;32m    360\u001b[0m         response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContentLength\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    363\u001b[0m download_output_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_download_output_manager_cls(\n\u001b[1;32m    364\u001b[0m     transfer_future, osutil\n\u001b[1;32m    365\u001b[0m )(osutil, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator, io_executor)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/client.py:531\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_api_call(operation_name, kwargs)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/client.py:947\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    946\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 947\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    948\u001b[0m         operation_model, request_dict, request_context\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    953\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, operation_name\u001b[38;5;241m=\u001b[39moperation_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/client.py:970\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint\u001b[38;5;241m.\u001b[39mmake_request(operation_model, request_dict)\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    973\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{operation_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    974\u001b[0m                 service_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m    979\u001b[0m         )\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_dict, operation_model)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/endpoint.py:198\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    196\u001b[0m context \u001b[38;5;241m=\u001b[39m request_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[0;32m--> 198\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[1;32m    199\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[1;32m    200\u001b[0m     request, operation_model, context\n\u001b[1;32m    201\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    203\u001b[0m     attempts,\n\u001b[1;32m    204\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     exception,\n\u001b[1;32m    208\u001b[0m ):\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/endpoint.py:134\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[1;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{service_id}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{op_name}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    132\u001b[0m         service_id\u001b[38;5;241m=\u001b[39mservice_id, op_name\u001b[38;5;241m=\u001b[39moperation_model\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    133\u001b[0m     )\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_emitter\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    135\u001b[0m         event_name,\n\u001b[1;32m    136\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[1;32m    137\u001b[0m         operation_name\u001b[38;5;241m=\u001b[39moperation_model\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_request\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emitter\u001b[38;5;241m.\u001b[39memit(aliased_event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emit(event_name, kwargs)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[0;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/signers.py:105\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msign(operation_name, request)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/signers.py:180\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigning_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m signing_context[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigning_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_auth_instance(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnknownSignatureVersionError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m signing_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/signers.py:284\u001b[0m, in \u001b[0;36mRequestSigner.get_auth_instance\u001b[0;34m(self, signing_name, region_name, signature_version, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m frozen_credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     frozen_credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials\u001b[38;5;241m.\u001b[39mget_frozen_credentials()\n\u001b[1;32m    285\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredentials\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m frozen_credentials\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mREQUIRES_REGION:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/credentials.py:3058\u001b[0m, in \u001b[0;36mOAuth2Credentials.get_frozen_credentials\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_frozen_credentials\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return immutable credentials.\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \n\u001b[1;32m   3027\u001b[0m \u001b[38;5;124;03m    The ``access_key``, ``secret_key``, and ``token`` properties\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3056\u001b[0m \n\u001b[1;32m   3057\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3058\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_manager\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m   3060\u001b[0m     \u001b[38;5;66;03m# Signer is only interested in token, and besides, we might not even have api key\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ReadOnlyCredentials(\n\u001b[1;32m   3062\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, token)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/credentials.py:2600\u001b[0m, in \u001b[0;36mDefaultTokenManager.get_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cache_token(): \u001b[38;5;66;03m# try again another thread may have refreshed it\u001b[39;00m\n\u001b[0;32m-> 2600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_token()\n\u001b[1;32m   2601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_token_set_event\u001b[38;5;241m.\u001b[39mset();\n\u001b[1;32m   2603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_background_thread:\n\u001b[1;32m   2604\u001b[0m             \u001b[38;5;66;03m# check to see if the thread is still running\u001b[39;00m\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/credentials.py:2796\u001b[0m, in \u001b[0;36mDefaultTokenManager._get_initial_token\u001b[0;34m(self, retry_count, retry_delay)\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   2795\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2796\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_function()\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_botocore/credentials.py:2685\u001b[0m, in \u001b[0;36mDefaultTokenManager._default_auth_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m httplib\u001b[38;5;241m.\u001b[39mOK:\n\u001b[1;32m   2684\u001b[0m     _msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHttpCode(\u001b[39m\u001b[38;5;132;01m{code}\u001b[39;00m\u001b[38;5;124m) - Retrieval of tokens from server failed.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m-> 2685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CredentialRetrievalError(provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_token_url(), error_msg\u001b[38;5;241m=\u001b[39m_msg)\n\u001b[1;32m   2687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n", "\u001b[0;31mCredentialRetrievalError\u001b[0m: Error when retrieving credentials from https://iam.cloud.ibm.com/identity/token: HttpCode(400) - Retrieval of tokens from server failed."], "ename": "CredentialRetrievalError", "evalue": "Error when retrieving credentials from https://iam.cloud.ibm.com/identity/token: HttpCode(400) - Retrieval of tokens from server failed.", "output_type": "error"}], "execution_count": 5}, {"cell_type": "code", "source": "import pandas as pd\nimport ibm_boto3\nfrom ibm_botocore.client import Config\nimport json\n\n# IBM Cloud Credentials - Replace these with your correct values\nCOS_API_KEY_ID = \"your-valid-api-key\"  # Replace with your valid API Key\nCOS_RESOURCE_CRN = \"your-instance-crn\"  # Replace with your correct instance CRN\nCOS_ENDPOINT = \"https://s3.us.cloud-object-storage.appdomain.cloud\"  # Replace if needed\nBUCKET_NAME = \"your-bucket-name\"  # Replace with your correct bucket name\nFILE_NAME = \"multi_source_dataset_.csv\"\n\n# Function to connect to IBM Cloud Object Storage\ndef connect_cos():\n    try:\n        cos = ibm_boto3.client(\n            \"s3\",\n            ibm_api_key_id=COS_API_KEY_ID,\n            ibm_service_instance_id=COS_RESOURCE_CRN,\n            config=Config(signature_version=\"oauth\"),\n            endpoint_url=COS_ENDPOINT\n        )\n        print(\"Connected to IBM Cloud Object Storage successfully.\")\n        return cos\n    except Exception as e:\n        print(f\"Error connecting to IBM COS: {e}\")\n        return None\n\n# Function to download the dataset\ndef download_file(cos_client):\n    try:\n        cos_client.download_file(BUCKET_NAME, FILE_NAME, FILE_NAME)\n        print(f\"Downloaded {FILE_NAME} successfully.\")\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")\n\n# Function to process dataset\ndef process_dataset():\n    try:\n        df = pd.read_csv(FILE_NAME)\n        print(df.head())  # Display first few rows\n        df_cleaned = df.dropna()  # Example processing: Remove missing values\n        df_cleaned.to_csv(\"processed_dataset.csv\", index=False)\n        print(\"Dataset processed and saved as 'processed_dataset.csv'.\")\n    except Exception as e:\n        print(f\"Error processing dataset: {e}\")\n\n# Function to upload processed file back to IBM Cloud\ndef upload_file(cos_client):\n    try:\n        processed_file_name = \"processed_dataset.csv\"\n        cos_client.upload_file(processed_file_name, BUCKET_NAME, processed_file_name)\n        print(f\"Uploaded {processed_file_name} successfully to {BUCKET_NAME}.\")\n    except Exception as e:\n        print(f\"Error uploading file: {e}\")\n\n# Main Execution\ncos_client = connect_cos()\nif cos_client:\n    download_file(cos_client)\n    process_dataset()\n    upload_file(cos_client)\n", "metadata": {"id": "34d35ec3-3dad-4c75-bc5c-4632237dbdb7"}, "outputs": [{"name": "stdout", "text": "Connected to IBM Cloud Object Storage successfully.\nError downloading file: Error when retrieving credentials from https://iam.cloud.ibm.com/identity/token: HttpCode(400) - Retrieval of tokens from server failed.\nError processing dataset: [Errno 2] No such file or directory: 'multi_source_dataset_.csv'\nError uploading file: [Errno 2] No such file or directory: 'processed_dataset.csv'\n", "output_type": "stream"}], "execution_count": 10}, {"cell_type": "code", "source": "import pandas as pd\nimport os\nfrom thefuzz import process\nfrom recordlinkage import Compare, Index\n\n# Define dataset path\ndataset_paths = ['/mnt/data/multi_source_dataset.csv', '/home/wsuser/work/multi_source_dataset.csv']\ndataset_path = next((path for path in dataset_paths if os.path.exists(path)), None)\n\nif not dataset_path:\n    raise FileNotFoundError(\"Dataset not found in expected locations. Please upload the dataset.\")\n\n# Load dataset\ndata = pd.read_csv(dataset_path)\nprint(f\"\u2705 Dataset Loaded Successfully from {dataset_path}\\n\", data.head())\n\n# Data Cleaning: Handle missing values and normalize text\ndef clean_text(text):\n    return str(text).strip().lower() if pd.notnull(text) else \"\"\n\ncolumns_to_clean = ['Name', 'Email', 'Phone_Number', 'Address']\nfor col in columns_to_clean:\n    data[col] = data[col].apply(clean_text)\n\ndata.drop_duplicates(inplace=True)\nprint(\"\u2705 Data Cleaned!\\n\", data.head())\n\n# Entity Resolution using Fuzzy Matching\ndef fuzzy_match(df, col, threshold=85):\n    \"\"\"Perform fuzzy matching within a column and return matched pairs.\"\"\"\n    matches = []\n    for val in df[col]:\n        best_match, score = process.extractOne(val, df[col])\n        if score >= threshold and val != best_match:\n            matches.append((val, best_match, score))\n    return pd.DataFrame(matches, columns=['Original', 'Matched', 'Score'])\n\nentity_matches = fuzzy_match(data, 'Name')\nprint(\"\u2705 Fuzzy Matching Results:\\n\", entity_matches.head())\n\n# Similarity Join using Record Linkage\nindexer = Index()\nindexer.full()\npairs = indexer.index(data)\n\ncompare = Compare()\ncompare.string('Name', 'Name', method='jarowinkler', threshold=0.85)\ncompare.exact('Email', 'Email')\ncompare.exact('Phone_Number', 'Phone_Number')\n\nsimilarity_scores = compare.compute(pairs, data)\nduplicate_pairs = similarity_scores[similarity_scores.sum(axis=1) > 1]\nprint(\"\u2705 Potential Duplicate Records:\\n\", duplicate_pairs)\n\n# Merge similar records by keeping the first occurrence\nmerged_data = data.drop(duplicate_pairs.index.get_level_values(1))\nprint(\"\u2705 Final Integrated Dataset:\\n\", merged_data.head())\n\n# Save the integrated dataset\noutput_path = '/mnt/data/integrated_dataset.csv'\nmerged_data.to_csv(output_path, index=False)\nprint(f\"\u2705 Data Integration Completed! Saved as '{output_path}'\")\n", "metadata": {"id": "556f7d8b-a54f-4c50-8b49-f4fa30f8498d"}, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m dataset_paths \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path)), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_path:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found in expected locations. Please upload the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n", "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset not found in expected locations. Please upload the dataset."], "ename": "FileNotFoundError", "evalue": "Dataset not found in expected locations. Please upload the dataset.", "output_type": "error"}], "execution_count": 11}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Sample raw data\ndata = {\n    \"Customer_ID\": [1, 2, 3, 4],\n    \"Name\": [\"John Doe\", \"Jon Doe\", \"Jane Smith\", \"J. Smith\"],\n    \"Email\": [\"johnd@example.com\", \"johnd@example.com\", \"janes@example.com\", \"j.smith@example.com\"],\n    \"Phone_Number\": [\"123-456-7890\", \"123-456-7890\", \"987-654-3210\", \"987-654-3210\"],\n    \"Address\": [\"123 Main St\", \"123 Main St\", \"456 Elm St\", \"456 Elm St\"],\n    \"Date_of_Birth\": [\"1980-05-15\", \"1980-05-15\", \"1992-07-22\", \"1992-07-22\"],\n    \"Gender\": [\"M\", \"M\", \"F\", \"F\"],\n    \"Loyalty_Score\": [100, 105, 200, 195],\n    \"Source_System\": [\"CRM\", \"POS\", \"CRM\", \"POS\"]\n}\n\ndf = pd.DataFrame(data)\n\ndef find_duplicates(df, threshold=85):\n    \"\"\"Identify duplicates using fuzzy matching on Name, Email, and Phone_Number\"\"\"\n    duplicates = []\n    for i, row in df.iterrows():\n        for j, compare_row in df.iterrows():\n            if i != j:\n                name_score = fuzz.ratio(row[\"Name\"], compare_row[\"Name\"])\n                email_match = row[\"Email\"] == compare_row[\"Email\"]\n                phone_match = row[\"Phone_Number\"] == compare_row[\"Phone_Number\"]\n                \n                if name_score >= threshold and (email_match or phone_match):\n                    duplicates.append((i, j))\n    return duplicates\n\ndef merge_duplicates(df, duplicates):\n    \"\"\"Merge duplicate records by averaging numeric fields and keeping unique values.\"\"\"\n    merged_indices = set()  # Keep track of already merged indices\n    \n    for i, j in duplicates:\n        if j in merged_indices:  \n            continue  # Skip if already merged\n        \n        for col in df.columns:\n            if col in [\"Loyalty_Score\"]:  # Numeric fields\n                df.at[i, col] = (df.at[i, col] + df.at[j, col]) // 2\n            else:  # Keep unique values\n                df.at[i, col] = df.at[i, col] if df.at[i, col] == df.at[j, col] else f\"{df.at[i, col]} / {df.at[j, col]}\"\n        \n        merged_indices.add(j)  # Mark the duplicate row for deletion\n    \n    df.drop(index=list(merged_indices), inplace=True)  # Drop merged rows\n    return df.reset_index(drop=True)  # Reset index\n\nduplicates = find_duplicates(df)\ncleaned_df = merge_duplicates(df, duplicates)\n\nprint(cleaned_df)\n", "metadata": {"id": "c8317f74-b902-4d99-ad6e-f22b5d756a1b"}, "outputs": [{"name": "stdout", "text": "  Customer_ID        Name                Email  Phone_Number     Address  \\\n0           3  Jane Smith    janes@example.com  987-654-3210  456 Elm St   \n1           4    J. Smith  j.smith@example.com  987-654-3210  456 Elm St   \n\n  Date_of_Birth Gender  Loyalty_Score Source_System  \n0    1992-07-22      F            200           CRM  \n1    1992-07-22      F            195           POS  \n", "output_type": "stream"}, {"name": "stderr", "text": "/tmp/wsuser/ipykernel_539/1529011113.py:45: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1 / 2' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  df.at[i, col] = df.at[i, col] if df.at[i, col] == df.at[j, col] else f\"{df.at[i, col]} / {df.at[j, col]}\"\n", "output_type": "stream"}], "execution_count": 13}, {"cell_type": "code", "source": "import pandas as pd\nfrom fuzzywuzzy import fuzz\n\n# Sample raw data\ndata = {\n    \"Customer_ID\": [1, 2, 3, 4],\n    \"Name\": [\"John Doe\", \"Jon Doe\", \"Jane Smith\", \"J. Smith\"],\n    \"Email\": [\"johnd@example.com\", \"johnd@example.com\", \"janes@example.com\", \"j.smith@example.com\"],\n    \"Phone_Number\": [\"123-456-7890\", \"123-456-7890\", \"987-654-3210\", \"987-654-3210\"],\n    \"Address\": [\"123 Main St\", \"123 Main St\", \"456 Elm St\", \"456 Elm St\"],\n    \"Date_of_Birth\": [\"1980-05-15\", \"1980-05-15\", \"1992-07-22\", \"1992-07-22\"],\n    \"Gender\": [\"M\", \"M\", \"F\", \"F\"],\n    \"Loyalty_Score\": [100, 105, 200, 195],\n    \"Source_System\": [\"CRM\", \"POS\", \"CRM\", \"POS\"]\n}\n\ndf = pd.DataFrame(data)\n\ndef find_duplicates(df, threshold=85):\n    \"\"\"Identify duplicates using fuzzy matching on Name, Email, and Phone_Number\"\"\"\n    duplicates = []\n    for i, row in df.iterrows():\n        for j, compare_row in df.iterrows():\n            if i != j:\n                name_score = fuzz.ratio(row[\"Name\"], compare_row[\"Name\"])\n                email_match = row[\"Email\"] == compare_row[\"Email\"]\n                phone_match = row[\"Phone_Number\"] == compare_row[\"Phone_Number\"]\n                \n                if name_score >= threshold and (email_match or phone_match):\n                    duplicates.append((i, j))\n    return duplicates\n\ndef merge_duplicates(df, duplicates):\n    \"\"\"Merge duplicate records by averaging numeric fields and keeping unique values.\"\"\"\n    merged_indices = set()  # Keep track of already merged indices\n    \n    for i, j in duplicates:\n        if j in merged_indices:  \n            continue  # Skip if already merged\n        \n        for col in df.columns:\n            if col == \"Customer_ID\":  # Keep the lower ID to maintain uniqueness\n                df.at[i, col] = min(df.at[i, col], df.at[j, col])\n            elif col in [\"Loyalty_Score\"]:  # Numeric fields\n                df.at[i, col] = (df.at[i, col] + df.at[j, col]) // 2\n            else:  # Keep unique values\n                df.at[i, col] = str(df.at[i, col]) if df.at[i, col] == df.at[j, col] else f\"{df.at[i, col]} / {df.at[j, col]}\"\n        \n        merged_indices.add(j)  # Mark the duplicate row for deletion\n    \n    df.drop(index=list(merged_indices), inplace=True)  # Drop merged rows\n    return df.reset_index(drop=True)  # Reset index\n\nduplicates = find_duplicates(df)\ncleaned_df = merge_duplicates(df, duplicates)\n\nprint(cleaned_df)\n", "metadata": {"id": "3d3d3bdb-d7a6-4b59-b1b3-0ea20b551218"}, "outputs": [{"name": "stdout", "text": "   Customer_ID        Name                Email  Phone_Number     Address  \\\n0            3  Jane Smith    janes@example.com  987-654-3210  456 Elm St   \n1            4    J. Smith  j.smith@example.com  987-654-3210  456 Elm St   \n\n  Date_of_Birth Gender  Loyalty_Score Source_System  \n0    1992-07-22      F            200           CRM  \n1    1992-07-22      F            195           POS  \n", "output_type": "stream"}], "execution_count": 14}, {"cell_type": "code", "source": "", "metadata": {"id": "c6bb3018-54a6-4e33-a78f-ce56fcaf3d6c"}, "outputs": [], "execution_count": null}]}